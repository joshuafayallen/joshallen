[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to American Government \n                \n            \n            \n                Pols 1101 | \n                Georgia State University\n                \n            \n            Introduction to American Government\n\n            \n                \n                \n                 Fall 2022\n                \n                \n                \n                 Summer 2022\n                \n                \n                \n                 Spring 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Political Science Research \n                \n            \n            \n                Pols 3800 | \n                Georgia State University\n                \n            \n            This course serves as an introduction research methods, casual inference, applied statistics, and using R. During the semester we will cover how to develop research questions, theory development, and how to answer questions about political phenomena\n\n            \n                \n                 \n                 Spring 2023\n                \n                \n                 \n                 Summer 2023\n                \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshops \n                \n            \n            \n                 | \n                Georgia State University\n                \n            \n            Research Data Services R Workshops\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                 \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to American Government \n                \n            \n            \n                Pols 1101 | \n                Georgia State University\n                \n            \n            Introduction to American Government\n\n            \n                \n                \n                 Fall 2022\n                \n                \n                \n                 Summer 2022\n                \n                \n                \n                 Spring 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Political Science Research \n                \n            \n            \n                Pols 3800 | \n                Georgia State University\n                \n            \n            This course serves as an introduction research methods, casual inference, applied statistics, and using R. During the semester we will cover how to develop research questions, theory development, and how to answer questions about political phenomena\n\n            \n                \n                 \n                 Spring 2023\n                \n                \n                 \n                 Summer 2023\n                \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshops \n                \n            \n            \n                 | \n                Georgia State University\n                \n            \n            Research Data Services R Workshops\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                 \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josh Allen",
    "section": "",
    "text": "Welcome to my website! My name is Josh Allen and I am a PhD Candidate in the Department of Political Science at Georgia State University. I earned my M.A. at Georgia State and am a proud Sonoma State University alum.\nMy research focuses on the impact of the Holocaust on contemporary political attitudes in Europe."
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html",
    "href": "blog/2024/translating-dplyr-to-polars/index.html",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "",
    "text": "I suppose at some point it is good to become more well versed in lots of tools. I have been python curious for about a year or so and I think it is important to use the tool best suited for the task. Also sometimes it is important to get out of your comfort zone. I am definitely somebody who is very comfortable in R and the tidyverse and use it for a lot of stuff. I have heard lots of ravings about polars specifically about its speed and similarities in intuition with the tidyverse. So I thought I would have a collection of code for myself and the people of the internet to reference.\nJust a disclaimer. This is really just me working through the similarities and is going to be based on the tidyintelligence’s blog post, Robert Mitchell’s blog post, and Emily Rieder’s blog post. In all honesty, this is just for me to smash them together to have a one-stop shop for myself. If you found this post over these resources I highly recommend you check out these resources."
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#group-by-and-summarize",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#group-by-and-summarize",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Group by and summarize",
    "text": "Group by and summarize\nLast but not least we need to do the group by and summarise bit. It looks like this is slightly more intuitive\n\nRPython\n\n\n\npenguins |&gt;\ngroup_by(species) |&gt;\nsummarise(total = n())\n\n# A tibble: 3 × 2\n  species   total\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\n\npenguins.group_by(pl.col(\"species\")).agg(total = pl.count())\n\n\n\nshape: (3, 2)\n\n\n\nspecies\ntotal\n\n\nstr\nu32\n\n\n\n\n\"Adelie\"\n152\n\n\n\"Gentoo\"\n124\n\n\n\"Chinstrap\"\n68\n\n\n\n\n\n\n\n\n\n\nLets do some mathy stuff\n\npenguins.group_by(pl.col(\"species\")).agg(count = pl.len(),\n                                         mean_flipp = pl.mean(\"flipper_length_mm\"),\n                                         median_flipp = pl.median(\"flipper_length_mm\"))\n\n\n\nshape: (3, 4)\n\n\n\nspecies\ncount\nmean_flipp\nmedian_flipp\n\n\nstr\nu32\nf64\nf64\n\n\n\n\n\"Gentoo\"\n124\n217.186992\n216.0\n\n\n\"Chinstrap\"\n68\n195.823529\n196.0\n\n\n\"Adelie\"\n152\n189.953642\n190.0\n\n\n\n\n\n\n\nA thing that is useful in summarize is that we can use our selectors to summarise across multiple columns like this\n\npenguins |&gt;\ngroup_by(species) |&gt;\nsummarise(across(starts_with(\"bill\"), list(mean = \\(x) mean(x, na.rm = TRUE,\n                                           median = \\(x) median(x, na.rm,  TRUE)))))\n\n# A tibble: 3 × 3\n  species   bill_length_mm_mean bill_depth_mm_mean\n  &lt;fct&gt;                   &lt;dbl&gt;              &lt;dbl&gt;\n1 Adelie                   38.8               18.3\n2 Chinstrap                48.8               18.4\n3 Gentoo                   47.5               15.0\n\n\nIn polars I imagine it would probably be something like this\n\npenguins.group_by(pl.col(\"species\")).agg(cs.starts_with(\"bill\").mean())\n\n\n\nshape: (3, 3)\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\n\n\nstr\nf64\nf64\n\n\n\n\n\"Adelie\"\n38.791391\n18.346358\n\n\n\"Gentoo\"\n47.504878\n14.982114\n\n\n\"Chinstrap\"\n48.833824\n18.420588\n\n\n\n\n\n\n\nThe think I am running into now is that I would like to add a _ without doing any extra work. It looks like according to the docs it should be this\n\npenguins.group_by(pl.col(\"species\")).agg(cs.starts_with(\"bill\").mean().name.suffix(\"_mean\"),\n                                         cs.starts_with(\"bill\").median().name.suffix(\"_median\"))\n\n\n\nshape: (3, 5)\n\n\n\nspecies\nbill_length_mm_mean\nbill_depth_mm_mean\nbill_length_mm_median\nbill_depth_mm_median\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"Adelie\"\n38.791391\n18.346358\n38.8\n18.4\n\n\n\"Chinstrap\"\n48.833824\n18.420588\n49.55\n18.45\n\n\n\"Gentoo\"\n47.504878\n14.982114\n47.3\n15.0"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#joins-in-polars",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#joins-in-polars",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Joins in Polars",
    "text": "Joins in Polars\nIt would be nice if we had all the data we wanted in one dataset but that is not life we often need to join data. Critically we also would not want to have all our data in one place if we care about users safety. So we may want to keep portions of the dataset in separate places. So lets define a simple dataset to work with.\n\nnational_data &lt;- tribble(\n  ~state, ~year, ~unemployment, ~inflation, ~population,\n  \"GA\",   2018,  5,             2,          100,\n  \"GA\",   2019,  5.3,           1.8,        200,\n  \"GA\",   2020,  5.2,           2.5,        300,\n  \"NC\",   2018,  6.1,           1.8,        350,\n  \"NC\",   2019,  5.9,           1.6,        375,\n  \"NC\",   2020,  5.3,           1.8,        400,\n  \"CO\",   2018,  4.7,           2.7,        200,\n  \"CO\",   2019,  4.4,           2.6,        300,\n  \"CO\",   2020,  5.1,           2.5,        400\n)\n\nnational_libraries &lt;- tribble(\n  ~state, ~year, ~libraries, ~schools,\n  \"CO\",   2018,  230,        470,\n  \"CO\",   2019,  240,        440,\n  \"CO\",   2020,  270,        510,\n  \"NC\",   2018,  200,        610,\n  \"NC\",   2019,  210,        590,\n  \"NC\",   2020,  220,        530,\n)\n\n\n\nnational_dict = {\"state\": [\"Ga\", \"Ga\", \"Ga\",  \"NC\", \"NC\", \"NC\", \"CO\", \"CO\", \"CO\"], \"unemployment\":[6,6,8,6,4,3,7,8,9], \"year\": [2019,2018,2017,2019,2018,2017,2019,2018,2017]}\n\n\nnational_data = pl.DataFrame(national_dict)\n\n\nlibrary_dict = {\"state\":[\"CO\", \"CO\", \"CO\"], \"libraries\": [23234,2343234,32342342], \"year\":[2019,2018,2017]}\n\n\nlibrary_data = pl.DataFrame(library_dict)\n\nWe may want to merge in the library dataset. In tidyland we would do something like this\n\nnational_data |&gt;\nleft_join(national_libraries, join_by(state, year))\n\n# A tibble: 9 × 7\n  state  year unemployment inflation population libraries schools\n  &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 GA     2018          5         2          100        NA      NA\n2 GA     2019          5.3       1.8        200        NA      NA\n3 GA     2020          5.2       2.5        300        NA      NA\n4 NC     2018          6.1       1.8        350       200     610\n5 NC     2019          5.9       1.6        375       210     590\n6 NC     2020          5.3       1.8        400       220     530\n7 CO     2018          4.7       2.7        200       230     470\n8 CO     2019          4.4       2.6        300       240     440\n9 CO     2020          5.1       2.5        400       270     510\n\n\nIn polars land we would join the data like this\n\njoined_data = national_data.join(library_data, on = [\"state\",\"year\"], how = \"left\")\n\njoined_data \n\n\n\nshape: (9, 4)\n\n\n\nstate\nunemployment\nyear\nlibraries\n\n\nstr\ni64\ni64\ni64\n\n\n\n\n\"Ga\"\n6\n2019\nnull\n\n\n\"Ga\"\n6\n2018\nnull\n\n\n\"Ga\"\n8\n2017\nnull\n\n\n\"NC\"\n6\n2019\nnull\n\n\n\"NC\"\n4\n2018\nnull\n\n\n\"NC\"\n3\n2017\nnull\n\n\n\"CO\"\n7\n2019\n23234\n\n\n\"CO\"\n8\n2018\n2343234\n\n\n\"CO\"\n9\n2017\n32342342\n\n\n\n\n\n\n\nThis is honestly pretty comfortable. One thing that is really nice about dplyr is that you can pretty easily join columns that are not named the same thing.\n\nnational_libraries = national_libraries |&gt;\nrename(state_name = state)\n\n\n\nnational_data |&gt;\nleft_join(national_libraries, join_by(state == state_name, year))\n\n# A tibble: 9 × 7\n  state  year unemployment inflation population libraries schools\n  &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 GA     2018          5         2          100        NA      NA\n2 GA     2019          5.3       1.8        200        NA      NA\n3 GA     2020          5.2       2.5        300        NA      NA\n4 NC     2018          6.1       1.8        350       200     610\n5 NC     2019          5.9       1.6        375       210     590\n6 NC     2020          5.3       1.8        400       220     530\n7 CO     2018          4.7       2.7        200       230     470\n8 CO     2019          4.4       2.6        300       240     440\n9 CO     2020          5.1       2.5        400       270     510\n\n\nIn polars the process is less clear immediately. Instead of a nice join_by argument you have specify the keys separately. But still pretty intuitive.\n\nlibrary_dat = library_data.rename({\"state\": \"state_name\"})\n\n\nnational_data.join(library_dat, left_on = [\"state\", \"year\"],\n               right_on = [\"state_name\", \"year\"], how = \"left\" )\n\n\n\nshape: (9, 4)\n\n\n\nstate\nunemployment\nyear\nlibraries\n\n\nstr\ni64\ni64\ni64\n\n\n\n\n\"Ga\"\n6\n2019\nnull\n\n\n\"Ga\"\n6\n2018\nnull\n\n\n\"Ga\"\n8\n2017\nnull\n\n\n\"NC\"\n6\n2019\nnull\n\n\n\"NC\"\n4\n2018\nnull\n\n\n\"NC\"\n3\n2017\nnull\n\n\n\"CO\"\n7\n2019\n23234\n\n\n\"CO\"\n8\n2018\n2343234\n\n\n\"CO\"\n9\n2017\n32342342"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#binding-rows",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#binding-rows",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Binding Rows",
    "text": "Binding Rows\nSometimes we just want to add rows to our data\n\na = data.frame(id = 1:2, vals = 1:2)\n\nb  = data.frame(id = 3:4, vals = 3:4)\n\n\na |&gt;\nbind_rows(b)\n\n  id vals\n1  1    1\n2  2    2\n3  3    3\n4  4    4\n\n\nor we want to add columns\n\nc = data.frame(chars = c(\"hello\", \"lorem\"),\n               var_23 = c(\"world\", \"ipsum\"))\n\na |&gt;\nbind_cols(c)\n\n  id vals chars var_23\n1  1    1 hello  world\n2  2    2 lorem  ipsum\n\n\nHow would we do this in polars?\n\na = pl.DataFrame(\n       {\"a\": [1,2],\n        \"b\": [3,4]}\n)\n\nb = pl.DataFrame({\"a\" : [3,4], \"b\": [5,6]})\n\n\npl.concat([a, b], how = \"vertical\")\n\n\n\nshape: (4, 2)\n\n\n\na\nb\n\n\ni64\ni64\n\n\n\n\n1\n3\n\n\n2\n4\n\n\n3\n5\n\n\n4\n6\n\n\n\n\n\n\n\nAgain fairly intuitive if we wanted to bind the columns\n\nc = pl.DataFrame({\"chars\": [\"hello\", \"lorem\"], \"chars2\":[\"world\",\"ipsum\"]})\n\n\npl.concat([a,c], how = \"horizontal\")\n\n\n\nshape: (2, 4)\n\n\n\na\nb\nchars\nchars2\n\n\ni64\ni64\nstr\nstr\n\n\n\n\n1\n3\n\"hello\"\n\"world\"\n\n\n2\n4\n\"lorem\"\n\"ipsum\""
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#pivots-of-all-shapes",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#pivots-of-all-shapes",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Pivots of all shapes",
    "text": "Pivots of all shapes\nSometimes we need to pivot our data. Lets use the built in example from tidyr. Basically we have a whole bunch of columns that denote counts of income brackets\n\nrelig = relig_income\n\n\nwrite_csv(relig,\"relig_income.csv\")\n\n\nhead(relig_income)\n\n# A tibble: 6 × 11\n  religion  `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic       27        34        60        81        76       137        122\n2 Atheist        12        27        37        52        35        70         73\n3 Buddhist       27        21        30        34        33        58         62\n4 Catholic      418       617       732       670       638      1116        949\n5 Don’t kn…      15        14        15        11        10        35         21\n6 Evangeli…     575       869      1064       982       881      1486        949\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nIn tidyr we would just do this\n\nrelig |&gt;\npivot_longer(-religion,\n              names_to = \"income_bracket\",\n              values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income_bracket     count\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\nwhich is nice because we can just identify a column and then pivot. One thing that I will have to just memorize is that when we are moving things to long in polars than we melt the dataframe. Kind of like a popsicle or something. The mnemonic device will come to me eventually\n\nrelig = pl.read_csv(\"relig_income.csv\")\n\nrelig.head()\n\n\n\nshape: (5, 11)\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\n$100-150k\n&gt;150k\nDon't know/refused\n\n\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\n\"Agnostic\"\n27\n34\n60\n81\n76\n137\n122\n109\n84\n96\n\n\n\"Atheist\"\n12\n27\n37\n52\n35\n70\n73\n59\n74\n76\n\n\n\"Buddhist\"\n27\n21\n30\n34\n33\n58\n62\n39\n53\n54\n\n\n\"Catholic\"\n418\n617\n732\n670\n638\n1116\n949\n792\n633\n1489\n\n\n\"Don’t know/ref…\n15\n14\n15\n11\n10\n35\n21\n17\n18\n116\n\n\n\n\n\n\n\nTo melt all we do is\n\nrelig.melt(id_vars = \"religion\", variable_name = \"income_bracket\", value_name = \"count\")\n\n\n\nshape: (180, 3)\n\n\n\nreligion\nincome_bracket\ncount\n\n\nstr\nstr\ni64\n\n\n\n\n\"Agnostic\"\n\"&lt;$10k\"\n27\n\n\n\"Atheist\"\n\"&lt;$10k\"\n12\n\n\n\"Buddhist\"\n\"&lt;$10k\"\n27\n\n\n\"Catholic\"\n\"&lt;$10k\"\n418\n\n\n\"Don’t know/ref…\n\"&lt;$10k\"\n15\n\n\n…\n…\n…\n\n\n\"Orthodox\"\n\"Don't know/ref…\n73\n\n\n\"Other Christia…\n\"Don't know/ref…\n18\n\n\n\"Other Faiths\"\n\"Don't know/ref…\n71\n\n\n\"Other World Re…\n\"Don't know/ref…\n8\n\n\n\"Unaffiliated\"\n\"Don't know/ref…\n597\n\n\n\n\n\n\n\nsame would go for the pivoting wider\n\npenguins.pivot(index = \"island\",columns = \"species\", values = \"body_mass_g\",\n              aggregate_function=\"sum\")\n\n\n\nshape: (3, 4)\n\n\n\nisland\nAdelie\nGentoo\nChinstrap\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"Torgersen\"\n189025.0\nnull\nnull\n\n\n\"Biscoe\"\n163225.0\n624350.0\nnull\n\n\n\"Dream\"\n206550.0\nnull\n253850.0\n\n\n\n\n\n\n\nthis isn’t quite the same because we are aggregating it. This is likely just a skill issue on the user end. But still we have wide data now!\n\nUsing selectors in pivot longer\nA slightly more complex example is using the billboards datas\n\nbillboards = tidyr::billboard\n\n\nwrite_csv(billboards, \"billboard.csv\")\n\n\nhead(billboards)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n billboards |&gt;\npivot_longer(cols = starts_with(\"wk\"),\n              names_to = \"week\",\n              values_to = \"count_of_weeks\")\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week  count_of_weeks\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt;          &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1               87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2               82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3               72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4               77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5               87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6               94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7               99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8               NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9               NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10              NA\n# ℹ 24,082 more rows\n\n\nWe can do something similar with polars by using our selectors.\n\nbillboards = pl.read_csv(\"billboard.csv\")\n\n\nbillboards.melt(id_vars = \"artist\",value_vars  = cs.starts_with(\"wk\"),\n                variable_name = \"week\", value_name = \"count\" )\n\n\n\nshape: (24_092, 3)\n\n\n\nartist\nweek\ncount\n\n\nstr\nstr\nstr\n\n\n\n\n\"2 Pac\"\n\"wk1\"\n\"87\"\n\n\n\"2Ge+her\"\n\"wk1\"\n\"91\"\n\n\n\"3 Doors Down\"\n\"wk1\"\n\"81\"\n\n\n\"3 Doors Down\"\n\"wk1\"\n\"76\"\n\n\n\"504 Boyz\"\n\"wk1\"\n\"57\"\n\n\n…\n…\n…\n\n\n\"Yankee Grey\"\n\"wk76\"\n\"NA\"\n\n\n\"Yearwood, Tris…\n\"wk76\"\n\"NA\"\n\n\n\"Ying Yang Twin…\n\"wk76\"\n\"NA\"\n\n\n\"Zombie Nation\"\n\"wk76\"\n\"NA\"\n\n\n\"matchbox twent…\n\"wk76\"\n\"NA\"\n\n\n\n\n\n\n\nBroadly it works the same but if you don’t specify the id vars you will end up with just the week and count column"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#unnest",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#unnest",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Unnest",
    "text": "Unnest\nSometimes we have these unfriendly list columns that we would like to make not lists. Lets go ahead and use the starwars list columns.\n\nstarwars_lists = starwars |&gt;\nselect(name, where(is.list)) |&gt;\nunnest_longer(starships , keep_empty = TRUE) |&gt;\nunnest_longer(films, keep_empty = TRUE) |&gt;\nunnest_longer(vehicles, keep_empty = TRUE)\n\n\n\n\n\nhead(starwars_lists)\n\n# A tibble: 6 × 4\n  name           films                   vehicles              starships\n  &lt;chr&gt;          &lt;chr&gt;                   &lt;chr&gt;                 &lt;chr&gt;    \n1 Luke Skywalker The Empire Strikes Back Snowspeeder           X-wing   \n2 Luke Skywalker The Empire Strikes Back Imperial Speeder Bike X-wing   \n3 Luke Skywalker Revenge of the Sith     Snowspeeder           X-wing   \n4 Luke Skywalker Revenge of the Sith     Imperial Speeder Bike X-wing   \n5 Luke Skywalker Return of the Jedi      Snowspeeder           X-wing   \n6 Luke Skywalker Return of the Jedi      Imperial Speeder Bike X-wing   \n\n\nIn polars we have a similarish function named explode. Unfortunately we don’t have a a selector for all attribute types so we are going to do this by hand.\n\nstarwars_list = starwars.select([\"name\", \"films\", \"vehicles\", \"starships\"])\n\nstarwars_list.glimpse()\n\nRows: 87\nColumns: 4\n$ name            &lt;str&gt; 'Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi'\n$ films     &lt;list[str]&gt; ['The Empire Strikes Back', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope', 'The Force Awakens'], ['The Empire Strikes Back', 'Attack of the Clones', 'The Phantom Menace', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope'], ['The Empire Strikes Back', 'Attack of the Clones', 'The Phantom Menace', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope', 'The Force Awakens'], ['The Empire Strikes Back', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope'], ['The Empire Strikes Back', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope', 'The Force Awakens'], ['Attack of the Clones', 'Revenge of the Sith', 'A New Hope'], ['Attack of the Clones', 'Revenge of the Sith', 'A New Hope'], ['A New Hope'], ['A New Hope'], ['The Empire Strikes Back', 'Attack of the Clones', 'The Phantom Menace', 'Revenge of the Sith', 'Return of the Jedi', 'A New Hope']\n$ vehicles  &lt;list[str]&gt; ['Snowspeeder', 'Imperial Speeder Bike'], [], [], [], ['Imperial Speeder Bike'], [], [], [], [], ['Tribubble bongo']\n$ starships &lt;list[str]&gt; ['X-wing', 'Imperial shuttle'], [], [], ['TIE Advanced x1'], [], [], [], [], ['X-wing'], ['Jedi starfighter', 'Trade Federation cruiser', 'Naboo star skiff', 'Jedi Interceptor', 'Belbullab-22 starfighter']\n\nstarwars_explode =  starwars_list.explode(\"films\").explode(\"vehicles\").explode(\"starships\")\n\nstarwars_explode.head()\n\n\n\nshape: (5, 4)\n\n\n\nname\nfilms\nvehicles\nstarships\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Luke Skywalker…\n\"The Empire Str…\n\"Snowspeeder\"\n\"X-wing\"\n\n\n\"Luke Skywalker…\n\"The Empire Str…\n\"Snowspeeder\"\n\"Imperial shutt…\n\n\n\"Luke Skywalker…\n\"The Empire Str…\n\"Imperial Speed…\n\"X-wing\"\n\n\n\"Luke Skywalker…\n\"The Empire Str…\n\"Imperial Speed…\n\"Imperial shutt…\n\n\n\"Luke Skywalker…\n\"Revenge of the…\n\"Snowspeeder\"\n\"X-wing\""
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#make-a-column-into-a-vector",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#make-a-column-into-a-vector",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Make a column into a vector",
    "text": "Make a column into a vector\nIn R there are like a ton of different ways to do this\n\nvec1 = penguins$bill_depth_mm\n\nvec2 = penguins |&gt;\npluck(\"bill_depth_mm\")\n\nvec3 = penguins |&gt;\nselect(bill_depth_mm) |&gt;\ndeframe()\n\nIn polars the equivalent of this\n\nvec1 = penguins[\"bill_depth_mm\"]\n\nprint(vec1[0,1])\n\nshape: (2,)\nSeries: 'bill_depth_mm' [f64]\n[\n    18.7\n    17.4\n]\n\n\n\nRPython\n\n\n\nvec1[1:3]\n\n[1] 18.7 17.4 18.0\n\n\n\n\n\nimport numpy as np \n\nprint(vec1[0:2])\n\nshape: (2,)\nSeries: 'bill_depth_mm' [f64]\n[\n    18.7\n    17.4\n]"
  },
  {
    "objectID": "blog/2022/2021-11-25-what-to-do-when-you-break-r/index.html",
    "href": "blog/2022/2021-11-25-what-to-do-when-you-break-r/index.html",
    "title": "What to do when you break R",
    "section": "",
    "text": "Hi all, when I first stated using R I tried making my website using blogdown. While Alison Hill PhD provides an excellent intro to launching your website. However, I am truly special and managed to mess this process up. For a few days whenever I did anything more computationally intensive than\n\nrm(list=ls())\nlibrary(tidyverse)\n\nI would get a nasty error message saying “c stack usage is too close to the limit” and I could not do anything. This would have been fine but at the time I was still taking classes and need to have R working to complete the problem sets.\nSo what did I do to get in the c stack death spiral and how did it end up being fixed? For the former you should not skip steps in Dr. Hill’s post. For the later well to spare you the long arduous process here is what we tried.\n\nme consulting stackoverflow & realizing I really f****k up\nrestarting my computer\nuninstalling & reinstalling blogdown\nuninstalling & reinstalling R\nuninstalling & reinstalling pandocs\n\nAfter hours of trouble shooting #rstats twitter came to the rescue when this distress signal was sent out.\n\n\n#rstats world! I have a student who is getting a \"c stack usage is too close to the limit\" every time when using knitr (even with an R-chunk-free Rmd file) & getting same error when trying to install anything with devtools or remotes. We've un/reinstalled R but no luck. Ideas?\n\n— Andrew Heiss (🐘 @andrew@fediscience.org) (@andrewheiss) February 7, 2021\n\n\nSo here is what ended up working. To start you will need a super simple Rmd file to test with in a local directory. I suggest starting a new Rmd file with nothing in it other than the default YAML header and “test” in the main body or “Lorem Ipsum” if you feel fancy.\nIn the terminal run the following code\n\ncd ~ \n\nls -la\n\nThen look for files starting with a period. Okay if you messed up in the initial blogdown setup you are looking for the “.Rprofile” that is causing you the problem. What ended up happening is that you broke all of R by including a recursive function. So restarting and uninstalling R will not kill the function it will be there\n\n\n\nWhat you are going to do is open a terminal in Rstudio or otherwise than start running this.\n\ncat.Rprofile\n\nthan run\n\ncat.zshrc\n\nthan after that run\nmv. Rprofile .Rprofile-original\nThen close out Rstudio and reopen Rstudio. Then try to knit your super simple Rmd file and install a package and doing something fun! Than knit that file.\nHopefully the dreaded C stack usage error is gone. If it is than celebrate\n\n\n\nbecause you can use R again!!!!"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2022/2022-03-11-using-r-to-streamline-midsemester-reports/index.html",
    "href": "blog/2022/2022-03-11-using-r-to-streamline-midsemester-reports/index.html",
    "title": "Streamlining Midsemester Reports With The Tidyverse",
    "section": "",
    "text": "At GSU, we have Early Alert that is meant to connect students with resources if they are not doing well in the first few weeks of classes. While setting up your rules of thumb is up to you, this can quickly soak up an entire day if you are going row by row in your class of 60 or more students. To streamline the process I turned to R because it is a fairly simple data cleaning task.\nOur learning management software likes to add lots of extra stuff to the column names in our data. While most of us would come up with a more concise name like Chapter_4 our software exports the column names as assignment name, grading scheme, and the weights that are assigned. So it looks like Chapter 4.2 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;. We could use janitor::clean_names to eliminate some of the extraneous stuff, however iCollege will get grumpy at us because they do not match the names in the gradebook. We could do this in Excel, and I have done it in the past, I figured I could speed this up in R while avoiding the headaches of ensuring each and every column is where it should be.\n\npacman::p_load(tidyverse, DT)\n\n\nThe downloaded binary packages are in\n    /var/folders/0k/jvdgmwf56zg1dw7n9mm2wzvc0000gp/T//RtmphLVZMW/downloaded_packages\n\nset.seed(1994)\n\nstudents = 26\n\ndat = tibble(id = 1:26,\n             Students = LETTERS,\n             `Chapter 4.2 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;` = rnorm(students, mean = 90, sd = 5),\n            `Chapter 4.3 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;` = rnorm(students, mean = 90, sd = 5),\n            `Chapter 4.4  Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;` = rnorm(students, mean = 90, sd = 5),\n            `Chapter 4.5  Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;` = rnorm(students, mean = 90, sd = 5),\n            `Exam 1 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams` = rnorm(students, mean = 85, sd = 3))  \n\nIn this simple example there are only 5 columns they have annoying names sure but it is not that bad. We can probably copy and paste them and we will be fine. However, in my real data there are 13 or so chapters with a few subsections in each of them. So this can get out from under us kind of quick and copy and pasting does not make our lives any easier. We also usually get columns that do not help us. Our ID variable is not doing anything other than providing the same info in a less transparent way than the student name variable, and more minor items like surveys which do not have a lot of weight on their final grade.\nI just used rnorm for convenience; however your data is more likely to have some missing values because students did not do stuff so it looks like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nStudents\nChapter 4.2 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.3 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.4 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.5 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nExam 1 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams\nsurveys\n\n\n\n\n1\nA\n83.56343\n81.70468\n85.71587\n85.97115\n83.71489\n100\n\n\n2\nB\n91.44406\n86.12473\n91.53892\n91.80590\n84.62108\n100\n\n\n3\nC\n98.50185\n89.76872\n91.79794\nNA\n85.11662\n100\n\n\n4\nD\n96.99304\n88.53903\n83.42538\n92.42625\n86.08429\n100\n\n\n5\nE\n90.49552\n92.64055\n93.27846\n89.24096\n80.39882\n100\n\n\n6\nF\n83.30126\n86.91250\n90.75522\n83.04509\n78.98100\n100\n\n\n7\nG\n94.03755\n81.34807\n87.57366\n99.04721\n80.11158\n100\n\n\n8\nH\n89.47185\n89.62792\n91.37094\n96.30433\n89.58399\n100\n\n\n9\nI\n92.78107\nNA\n96.44479\n87.20809\n81.31118\n100\n\n\n10\nJ\n83.04019\n88.74052\nNA\n96.58848\n85.93511\n100\n\n\n11\nK\n91.57146\nNA\n87.94661\n93.73501\n88.43763\n100\n\n\n12\nL\n94.18791\n89.64253\n90.53395\n88.29499\n86.82630\n100\n\n\n13\nM\n95.96341\n94.94366\n91.53490\n85.17583\n82.57437\n100\n\n\n14\nN\n84.91042\n90.50202\n93.41616\n87.64364\n83.46703\n100\n\n\n15\nO\n92.76743\n81.32158\n84.05944\n103.32883\n87.00560\n100\n\n\n16\nP\n92.48953\n93.73259\n91.46155\n93.64285\n87.11260\n100\n\n\n17\nQ\n91.88890\n91.99284\n95.01500\n95.61172\n84.63614\n100\n\n\n18\nR\n87.25405\n89.89440\n91.07664\n91.93496\n89.64706\n100\n\n\n19\nS\n83.62513\n82.48662\nNA\n92.74020\n90.45662\n100\n\n\n20\nT\n87.91310\n92.87887\n95.78937\n84.74539\n86.05332\n100\n\n\n21\nU\n91.36955\n96.67913\n90.59384\n87.86449\n79.76733\n100\n\n\n22\nV\n91.30800\n87.35611\n88.29575\n90.40188\n89.97015\n100\n\n\n23\nW\nNA\n89.35205\n97.02888\nNA\n82.47724\n100\n\n\n24\nX\n94.06035\n83.28452\n97.71377\n82.26974\n90.26051\n100\n\n\n25\nY\nNA\n91.24920\n96.15636\n88.80131\n90.26133\n100\n\n\n26\nZ\n94.57923\n93.57157\n91.66163\n91.83234\n84.33456\n100\n\n\n\n\n\nSo for the purpose of the report I treat NA’s as zeros. If you are dealing with multiple columns this is a pretty easy step just use mutate(across) and using some combination of starts_with, contains, everything, or ends_with to achieve the desired goal.\n\nimputed =  dat_miss %&gt;%\n  mutate(across(c(starts_with(\"exam\"), starts_with(\"chapter\"), \n                  ), ~replace(., is.na(.), 0)))\n\nSo that should take care of the NA's but we still need to generate our indicators. The assignments that carry the most weight are exams and the chapters, so I focus the most on those. In my use case, taking the sum makes sense, but for yours the average is probably the better option. Thankfully, while the learning management software names are a bit cumbersome, they do share something in common. So we can use mutate(across) and rowwise to make our life easier. rowwise is a pretty neat little function that works perfectly for this task where you are trying to do things for each student. Then you can use case_when or ifelse to generate a logical to create your flag. This is a toy example, but we can quickly start to build it out for our specific use cases. Using a mixture of apply and select you can achieve the same thing.\n\n flag = imputed %&gt;% \n  rowwise() %&gt;% \n  mutate(flag_dplyr = round(sum(across(starts_with(\"chapter\")))))\n\n\nflag$flag_apply = imputed %&gt;% \n  select(contains(\"chapter\")) %&gt;% \n  apply(., 1, function(row){\n    round(sum(row))\n  })\n\nCool, we can use this for our basic stuff, but I tend to weigh exams by how well students did. So your highest exam score counts for more, and your lowest exam has the least amount of weight. As with lots of things in R you can do this a few ways. There is probably a more concise way of doing this with apply it is ugly but works.\n\nexams_complete = imputed %&gt;% \n  mutate(`Exam 2 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams` = rnorm(students, mean = 70, sd = 11),\n         `Exam 3 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams` = rnorm(students, mean = 75, sd = 11))\n\n\n\nexams_complete$higest_exam_apply = exams_complete %&gt;%  \n  select(contains(\"Exam\")) %&gt;% \n  apply(., 1, function(row){\n    round(max(row))\n  })\n\n\nexams_complete = exams_complete %&gt;% \n  rowwise() %&gt;% \n  mutate(hig_exam_dplyr = round(max(c_across(contains(\"Exam\")))))\n\nexams_complete$lowest_exam_apply = exams_complete %&gt;%  \n  select(contains(\"Exam\")) %&gt;% \n  apply(., 1, function(row){\n    round(min(row))\n  })\n\nexams_complete = exams_complete %&gt;% \n  rowwise() %&gt;% \n  mutate(low_exam_dplyr = round(min(c_across(contains(\"Exam\")))))\n\nSo this is easy enough because we are just changing what we are doing with our summary function, but what about the second highest exam score? In this case you are going to have to use some trickery to get what you want\n\nexams_complete$second_highest = exams_complete %&gt;% \n  select(starts_with(\"Exam\")) %&gt;% \n  apply(., 1, function(row){\n    round(sort(row, decreasing = TRUE)[2])\n  })\n\nThis is simple enough and you can just use select and filter to get the info you want. However, as we all know we have to do some grading. You can use all your favorite dplyr tricks to grade and impute grades. This is the easy part, and now you can start to expand this out to using R to automate calculating grades. One super neat assignment all Intro to Government students at Georgia State do is assigning an adult field trip of sorts that is free for them. The students go to the National Center for Civil and Human Rights and do a tour and simulation of the lunch counter sit-ins. There are a few components to this: they submit a unique code as part of the proof that they have done it. Naturally, as is the case, some students just did not do it, but that nbd just use our friend left join, but to retain all the students be sure to include keep = TRUE so each student gets graded.\n\ncodes_data = tibble(id = 1:10,\n             Students = LETTERS[1:10],\n             code = 100)\n\n\nexams_complete = mutate(exams_complete, Code = NA)\n\n\ngrades_with_codes = left_join(exams_complete, codes_data, by = \"Students\", keep = TRUE) %&gt;% \n  mutate(Code = code,\n         Code = ifelse(is.na(Code), 0, Code)) %&gt;% \n  rename(id = id.x,\n         students = Students.x) %&gt;% \n  select(-id.y, -Students.y, -code)\n\nIn the real data, I join by using last names, which works for the most part. But you may need to check to make sure that your LMS has correctly spelled your students’ last name or, just as importantly, they spelled their last name correctly. Hopefully, this helps somebody. If not, at least it is tucked in a nice blog post. Be sure to check everything to make sure it is working, but if it works correctly then hopefully , you get a nice graded dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nstudents\nChapter 4.2 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.3 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.4 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nChapter 4.5 Points Grade &lt;Numeric MaxPoints:100 Weight:3.125 Category:Area9 Chapters CategoryWeight:23&gt;\nExam 1 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams\nsurveys\nExam 2 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams\nExam 3 Points Grade &lt;Numeric MaxPoints:55 Weight 10 Category: Exams\nhigest_exam_apply\nhig_exam_dplyr\nlowest_exam_apply\nlow_exam_dplyr\nsecond_highest\nCode\n\n\n\n\n1\nA\n83.56343\n81.70468\n85.71587\n85.97115\n83.71489\n100\n58.06166\n68.83867\n84\n84\n58\n58\n69\n100\n\n\n2\nB\n91.44406\n86.12473\n91.53892\n91.80590\n84.62108\n100\n69.96645\n73.51084\n85\n85\n70\n70\n74\n100\n\n\n3\nC\n98.50185\n89.76872\n91.79794\n0.00000\n85.11662\n100\n66.30306\n74.42536\n85\n85\n66\n66\n74\n100\n\n\n4\nD\n96.99304\n88.53903\n83.42538\n92.42625\n86.08429\n100\n65.79288\n85.94456\n86\n86\n66\n66\n86\n100\n\n\n5\nE\n90.49552\n92.64055\n93.27846\n89.24096\n80.39882\n100\n88.72094\n64.87935\n89\n89\n65\n65\n80\n100\n\n\n6\nF\n83.30126\n86.91250\n90.75522\n83.04509\n78.98100\n100\n80.41976\n73.06732\n80\n80\n73\n73\n79\n100\n\n\n7\nG\n94.03755\n81.34807\n87.57366\n99.04721\n80.11158\n100\n86.56743\n66.79291\n87\n87\n67\n67\n80\n100\n\n\n8\nH\n89.47185\n89.62792\n91.37094\n96.30433\n89.58399\n100\n87.48637\n74.13581\n90\n90\n74\n74\n87\n100\n\n\n9\nI\n92.78107\n0.00000\n96.44479\n87.20809\n81.31118\n100\n69.63665\n68.36826\n81\n81\n68\n68\n70\n100\n\n\n10\nJ\n83.04019\n88.74052\n0.00000\n96.58848\n85.93511\n100\n69.71254\n71.65925\n86\n86\n70\n70\n72\n100\n\n\n11\nK\n91.57146\n0.00000\n87.94661\n93.73501\n88.43763\n100\n71.99278\n78.33438\n88\n88\n72\n72\n78\n0\n\n\n12\nL\n94.18791\n89.64253\n90.53395\n88.29499\n86.82630\n100\n66.30946\n99.16532\n99\n99\n66\n66\n87\n0\n\n\n13\nM\n95.96341\n94.94366\n91.53490\n85.17583\n82.57437\n100\n71.14509\n62.94670\n83\n83\n63\n63\n71\n0\n\n\n14\nN\n84.91042\n90.50202\n93.41616\n87.64364\n83.46703\n100\n66.72115\n67.17081\n83\n83\n67\n67\n67\n0\n\n\n15\nO\n92.76743\n81.32158\n84.05944\n103.32883\n87.00560\n100\n88.98898\n66.12589\n89\n89\n66\n66\n87\n0\n\n\n16\nP\n92.48953\n93.73259\n91.46155\n93.64285\n87.11260\n100\n63.42139\n77.70564\n87\n87\n63\n63\n78\n0\n\n\n17\nQ\n91.88890\n91.99284\n95.01500\n95.61172\n84.63614\n100\n70.82987\n53.49421\n85\n85\n53\n53\n71\n0\n\n\n18\nR\n87.25405\n89.89440\n91.07664\n91.93496\n89.64706\n100\n69.95649\n86.20525\n90\n90\n70\n70\n86\n0\n\n\n19\nS\n83.62513\n82.48662\n0.00000\n92.74020\n90.45662\n100\n78.62938\n64.34364\n90\n90\n64\n64\n79\n0\n\n\n20\nT\n87.91310\n92.87887\n95.78937\n84.74539\n86.05332\n100\n84.55215\n80.42847\n86\n86\n80\n80\n85\n0\n\n\n21\nU\n91.36955\n96.67913\n90.59384\n87.86449\n79.76733\n100\n75.08180\n64.40788\n80\n80\n64\n64\n75\n0\n\n\n22\nV\n91.30800\n87.35611\n88.29575\n90.40188\n89.97015\n100\n68.53960\n79.79600\n90\n90\n69\n69\n80\n0\n\n\n23\nW\n0.00000\n89.35205\n97.02888\n0.00000\n82.47724\n100\n88.95738\n70.28810\n89\n89\n70\n70\n82\n0\n\n\n24\nX\n94.06035\n83.28452\n97.71377\n82.26974\n90.26051\n100\n63.12942\n78.58372\n90\n90\n63\n63\n79\n0\n\n\n25\nY\n0.00000\n91.24920\n96.15636\n88.80131\n90.26133\n100\n88.82365\n86.03999\n90\n90\n86\n86\n89\n0\n\n\n26\nZ\n94.57923\n93.57157\n91.66163\n91.83234\n84.33456\n100\n65.54887\n82.58063\n84\n84\n66\n66\n83\n0"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "March 11, 2022\n        \n        \n            Streamlining Midsemester Reports With The Tidyverse\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n\n            Working with LMS data using the tidyverse\n        \n        \n    \n    \n    \n                  \n            January 1, 2022\n        \n        \n            What to do when you break R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    zsh\n                \n                \n            \n            \n\n            What happens when you keep getting c stack usage errors \n        \n        \n    \n    \n\n\nNo matching items\n\n\n\n\n    \n    \n                  \n            March 12, 2024\n        \n        \n            Translating What I know in the tidyverse to polars:\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    python\n                \n                \n                \n                    polars\n                \n                \n            \n            \n\n            This is me learning the snake language\n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "March 11, 2022\n        \n        \n            Streamlining Midsemester Reports With The Tidyverse\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n\n            Working with LMS data using the tidyverse\n        \n        \n    \n    \n    \n                  \n            January 1, 2022\n        \n        \n            What to do when you break R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    zsh\n                \n                \n            \n            \n\n            What happens when you keep getting c stack usage errors \n        \n        \n    \n    \n\n\nNo matching items\n\n\n\n\n    \n    \n                  \n            March 12, 2024\n        \n        \n            Translating What I know in the tidyverse to polars:\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    python\n                \n                \n                \n                    polars\n                \n                \n            \n            \n\n            This is me learning the snake language\n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research and Projects",
    "section": "",
    "text": "Allen Joshua. “The Use of The Holocaust in Online Discourse and in Media: A Computational Approach”\nAllen Joshua. “Collective Memory and Contemporary Political Behavior: Evidence from France”"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research and Projects",
    "section": "",
    "text": "Allen Joshua. “The Use of The Holocaust in Online Discourse and in Media: A Computational Approach”\nAllen Joshua. “Collective Memory and Contemporary Political Behavior: Evidence from France”"
  },
  {
    "objectID": "research/index.html#dormant-papers",
    "href": "research/index.html#dormant-papers",
    "title": "Research and Projects",
    "section": "Dormant Papers",
    "text": "Dormant Papers\n\nAllen Joshua. Testing The Effects of U.S. Airstrikes on Insurgent Initiated Violence in Yemen"
  }
]